{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "satellite-winner",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "This tutorial will show you the basics of training and evaluating an instance of ``globalemu``. If you are just interested in evaluating the released models then take a look at the second part towards the bottom of the page. If you are intending to work with neutral fraction histories then the frame work for training and evaluating models is identical you just need to pass the kwarg ``xHI=True`` to all of the ``globalemu`` functions.\n",
    "\n",
    "## Training A Model\n",
    "\n",
    "This tutorial will show you how to train a ``globalemu`` model on simulations of the Global 21-cm signal.\n",
    "\n",
    "The first thing we need to do is download some 21-cm signal models to train our network on. For this we will use the 21cmGEM models and the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "productive-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "data_dir = 'downloaded_data/'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "files = ['Par_test_21cmGEM.txt', 'Par_train_21cmGEM.txt', 'T21_test_21cmGEM.txt', 'T21_train_21cmGEM.txt']\n",
    "saves = ['test_data.txt', 'train_data.txt', 'test_labels.txt', 'train_labels.txt']\n",
    "\n",
    "for i in range(len(files)):\n",
    "    url = 'https://zenodo.org/record/4541500/files/' + files[i]\n",
    "    with open(data_dir + saves[i], 'wb') as f:\n",
    "        f.write(requests.get(url).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-stranger",
   "metadata": {},
   "source": [
    "In order for ``globalemu`` to work the training data needs to be saved in the data_dir and in the files 'train_data.txt' and 'train_labels.txt' which are the inputs and outputs of the network respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-class",
   "metadata": {},
   "source": [
    "Once the files have been downloaded we can go ahead and perform the preprocessing necessary for ``globalemu`` to effectively train a model. We do this with the ``process()`` function found in ``globalemu.preprocess``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "brutal-rocket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing started...\n",
      "...preprocessing done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<globalemu.preprocess.process at 0x7f52947840a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from globalemu.preprocess import process\n",
    "\n",
    "base_dir = 'results/'\n",
    "z = np.linspace(5, 50, 451)\n",
    "num = 1000\n",
    "\n",
    "process(num, z, base_dir=base_dir, data_location=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-piano",
   "metadata": {},
   "source": [
    "Since this tutorial is only ment to demonstrate how to train a model with the ``globalemu`` code we are only going to pre-process 1000 models and train with 1000 models out of a possible ~24000. We do this by setting ``num=1000`` above but if we wanted to train on all the models we would set ``num='full'``.\n",
    "\n",
    "Importantly the pre-processing function takes the data in ``data_dir`` and saves a ``.csv`` file in the ``base_dir`` containing the preprocessed inputs for the neural network. It also saves some files used for normalisation in the ``base_dir`` so that when evaluating the network the inputs and outputs can be properly dealt with.\n",
    "\n",
    "Once pre-processing has been performed we can train our network with the ``nn()`` function in ``globalemu.network``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "intimate-vienna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 0.86669, RMSE: 0.91977, Time: 7.449\n",
      "Epoch: 001, Loss: 0.71657, RMSE: 0.83905, Time: 7.263\n",
      "Epoch: 002, Loss: 0.68185, RMSE: 0.81792, Time: 7.041\n",
      "Epoch: 003, Loss: 0.58273, RMSE: 0.75411, Time: 7.458\n",
      "Epoch: 004, Loss: 0.44889, RMSE: 0.66294, Time: 7.219\n",
      "Epoch: 005, Loss: 0.38274, RMSE: 0.61412, Time: 7.527\n",
      "Epoch: 006, Loss: 0.33968, RMSE: 0.57892, Time: 6.846\n",
      "Epoch: 007, Loss: 0.29800, RMSE: 0.54232, Time: 6.744\n",
      "Epoch: 008, Loss: 0.25744, RMSE: 0.50423, Time: 6.779\n",
      "Epoch: 009, Loss: 0.22223, RMSE: 0.46855, Time: 7.068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<globalemu.network.nn at 0x7f52950a4160>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from globalemu.network import nn\n",
    "\n",
    "nn(batch_size=451, epochs=10, base_dir=base_dir, layer_sizes=[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-hawaii",
   "metadata": {},
   "source": [
    "``nn()`` has a bunch of keyword arguments that can be passed if required. All are documented and all have default values. However you will likely need to change things like ``base_dir`` which tells the code where the pre-processed data is and also ``layer_sizes`` which determines the network architecture. ``epochs`` is the number of training calls and often the default will be insufficient for training the network.\n",
    "\n",
    "The code saves the model and loss history every ten epochs in case your computer crashes or the program is interrupted for some unforeseen reason. If this happens or you reach the max number of epochs and need to continue training you can do the following and the code will resume from the last save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "plastic-windows",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 0.19733, RMSE: 0.44147, Time: 6.970\n",
      "Epoch: 001, Loss: 0.17772, RMSE: 0.41894, Time: 7.025\n",
      "Epoch: 002, Loss: 0.16563, RMSE: 0.40436, Time: 6.879\n",
      "Epoch: 003, Loss: 0.15861, RMSE: 0.39549, Time: 6.773\n",
      "Epoch: 004, Loss: 0.15378, RMSE: 0.38935, Time: 6.674\n",
      "Epoch: 005, Loss: 0.15075, RMSE: 0.38541, Time: 6.850\n",
      "Epoch: 006, Loss: 0.14835, RMSE: 0.38228, Time: 6.881\n",
      "Epoch: 007, Loss: 0.14658, RMSE: 0.37993, Time: 6.768\n",
      "Epoch: 008, Loss: 0.14507, RMSE: 0.37788, Time: 6.810\n",
      "Epoch: 009, Loss: 0.14341, RMSE: 0.37561, Time: 6.802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<globalemu.network.nn at 0x7f5238d061f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn(batch_size=451, epochs=10, base_dir=base_dir, layer_sizes=[8], resume=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-associate",
   "metadata": {},
   "source": [
    "You have now successfully trained an instance of globalemu.\n",
    "\n",
    "## Evaluating Your Model\n",
    "\n",
    "We can go ahead and evaluate the model using the testing data that we downloaded earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "young-heath",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.loadtxt(data_dir + 'test_data.txt')\n",
    "test_labels = np.loadtxt(data_dir + 'test_labels.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-building",
   "metadata": {},
   "source": [
    "With the data loaded we will look at how the model performs when predicting\n",
    "the first signal in the data set. We do this with the ``evaluate()`` class\n",
    "in ``globalemu.eval`` which takes in a set of parameters and returns a signal.\n",
    "The class must first, however, be initialised with a set of kwargs.\n",
    "We supply a ``base_dir`` which contains the pre-processed data,\n",
    "normalisation factors and trained model. You can also pass a redshift range with the\n",
    "``z`` kwarg however if this isn't supplied than the function will return the\n",
    "signal at the original redshifts that were used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "european-nature",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0eb8c51d6ecb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrue_signal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'parameters'"
     ]
    }
   ],
   "source": [
    "from globalemu.eval import evaluate\n",
    "\n",
    "input_params = test_data[0, :]\n",
    "true_signal = test_labels[0, :]\n",
    "\n",
    "predictor = evaluate(base_dir=base_dir)\n",
    "signal, z = predictor(input_params)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(z, true_signal, label='True Signal')\n",
    "plt.plot(z, signal, label='Emulation')\n",
    "plt.legend()\n",
    "plt.ylabel(r'$\\delta T$ [mK]')\n",
    "plt.xlabel(r'$z$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-record",
   "metadata": {},
   "source": [
    "The emulation is pretty poor for several reasons; we didn't run the training for long enough (only 20 epochs), the network size is small and we used very little of the available training data.\n",
    "\n",
    "We can have a look at the same signal emulated with the released model on github. This was trained with a much more appropriately sized network, the full training data and a few hundred epochs. The results are therefore more similar to the true signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-lawrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = evaluate(base_dir='../T_release/')\n",
    "signal, z = predictor(input_params)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(z, true_signal, label='True Signal')\n",
    "plt.plot(z, signal, label='Emulation')\n",
    "plt.legend()\n",
    "plt.ylabel(r'$\\delta T$ [mK]')\n",
    "plt.xlabel(r'$z$')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
